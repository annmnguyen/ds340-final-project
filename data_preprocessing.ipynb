{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXjYlgugnd4O"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import dask\n",
        "import dask.bag as db\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "file_path = '/content/drive/My Drive/ds340/total.csv'\n",
        "\n",
        "total = pd.read_csv(file_path)"
      ],
      "metadata": {
        "id": "rHWhL8q4nlCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if total.groupby('track_genre').size().min() >= 100:\n",
        "    sampled_df = total.groupby('track_genre').sample(n=100, random_state=42)\n",
        "else:\n",
        "    print(\"Some genres do not have enough tracks. Adjusting sample size.\")\n",
        "    sampled_df = total.groupby('track_genre').apply(lambda x: x.sample(n=min(len(x), 100), random_state=42)).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "3dgTTjMfnl6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_df.to_csv('/content/drive/My Drive/ds340/sampled_df.csv')"
      ],
      "metadata": {
        "id": "0oU_xF6Qnt5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import requests\n",
        "import dask.bag as db\n",
        "\n",
        "# Path and directory setup\n",
        "dataset_path = '/content/drive/MyDrive/ds340/sampled_files'\n",
        "if not os.path.exists(dataset_path):\n",
        "    os.makedirs(dataset_path)\n",
        "\n",
        "def download_song(url, trackid, dataset_path):\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "        if response.status_code == 200:\n",
        "            filename = f\"{trackid}.mp3\"\n",
        "            file_path = os.path.join(dataset_path, filename)\n",
        "            with open(file_path, 'wb') as f:\n",
        "                f.write(response.content)\n",
        "            return file_path\n",
        "        else:\n",
        "            print(f\"Failed to download {url}: HTTP {response.status_code}\")\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Request failed for {url}: {str(e)}\")\n",
        "    return None\n",
        "\n",
        "def download_batch(batch):\n",
        "    # Downloads all songs in a batch\n",
        "    return [download_song(row['url'], row['track_id'], dataset_path) for row in batch]\n",
        "\n",
        "\n",
        "batch_size = 600\n",
        "batches = [sampled_df.iloc[i:i + batch_size].to_dict(orient='records') for i in range(0, len(sampled_df), batch_size)]\n",
        "\n",
        "# Using Dask to process batches\n",
        "downloaded_files = db.from_sequence(batches).map(download_batch)\n",
        "downloaded_file_paths = downloaded_files.compute()  # Flattening list of lists\n",
        "downloaded_file_paths = [item for sublist in downloaded_file_paths for item in sublist if item is not None]"
      ],
      "metadata": {
        "id": "sr9Ahk5Box1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import os\n",
        "\n",
        "def process_audio(file_path):\n",
        "    # Extract track_id from filename\n",
        "    track_id = os.path.basename(file_path).replace('.mp3', '')\n",
        "\n",
        "    # Load the audio file\n",
        "    y, sr = librosa.load(file_path, sr=22050)  # Load audio at the default sampling rate of 22050 Hz\n",
        "\n",
        "    # Compute MFCC features\n",
        "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13, n_fft=2048, hop_length=512)\n",
        "    #mfcc = np.asarray(mfcc, dtype=np.float32)\n",
        "    # Transpose the MFCC result\n",
        "    mfcc = mfcc.T\n",
        "\n",
        "    # Return both track_id and MFCC as a tuple for further processing\n",
        "    return (track_id, mfcc.tolist())\n"
      ],
      "metadata": {
        "id": "Y8V7tgIJozvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = '/content/drive/MyDrive/ds340/sampled_files'\n",
        "sampled_df = pd.read_csv('/content/drive/My Drive/ds340/sampled_df.csv')\n",
        "sampled_files = os.listdir(folder_path)\n",
        "\n",
        "track_ids = [os.path.splitext(filename)[0] for filename in sampled_files]\n",
        "\n",
        "filtered_df = sampled_df[sampled_df['track_id'].isin(track_ids)]\n",
        "\n",
        "filtered_df"
      ],
      "metadata": {
        "id": "e7gCiJhjofQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import requests\n",
        "import dask.bag as db\n",
        "\n",
        "# Path and directory setup\n",
        "dataset_path = '/content/drive/MyDrive/ds340/sampled_files'\n",
        "if not os.path.exists(dataset_path):\n",
        "    os.makedirs(dataset_path)\n",
        "\n",
        "def download_song(url, trackid, dataset_path):\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "        if response.status_code == 200:\n",
        "            filename = f\"{trackid}.mp3\"\n",
        "            file_path = os.path.join(dataset_path, filename)\n",
        "            with open(file_path, 'wb') as f:\n",
        "                f.write(response.content)\n",
        "            return file_path\n",
        "        else:\n",
        "            print(f\"Failed to download {url}: HTTP {response.status_code}\")\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Request failed for {url}: {str(e)}\")\n",
        "    return None\n",
        "\n",
        "def download_batch(batch):\n",
        "    # Downloads all songs in a batch\n",
        "    return [download_song(row['url'], row['track_id'], dataset_path) for row in batch]\n",
        "\n",
        "\n",
        "batch_size = 600\n",
        "batches = [sampled_df.iloc[i:i + batch_size].to_dict(orient='records') for i in range(0, len(sampled_df), batch_size)]\n",
        "\n",
        "# Using Dask to process batches\n",
        "downloaded_files = db.from_sequence(batches).map(download_batch)\n",
        "downloaded_file_paths = downloaded_files.compute()  # Flattening list of lists\n",
        "downloaded_file_paths = [item for sublist in downloaded_file_paths for item in sublist if item is not None]"
      ],
      "metadata": {
        "id": "v4bQSsy5nvhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import os\n",
        "\n",
        "def process_audio(file_path):\n",
        "    # Extract track_id from filename\n",
        "    track_id = os.path.basename(file_path).replace('.mp3', '')\n",
        "\n",
        "    # Load the audio file\n",
        "    y, sr = librosa.load(file_path, sr=22050)  # Load audio at the default sampling rate of 22050 Hz\n",
        "\n",
        "    # Compute MFCC features\n",
        "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13, n_fft=2048, hop_length=512)\n",
        "    #mfcc = np.asarray(mfcc, dtype=np.float32)\n",
        "    # Transpose the MFCC result\n",
        "    mfcc = mfcc.T\n",
        "\n",
        "    # Return both track_id and MFCC as a tuple for further processing\n",
        "    return (track_id, mfcc.tolist())\n"
      ],
      "metadata": {
        "id": "owJvvkE-nzKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List all files in the specified directory and filter for MP3 files\n",
        "downloaded_file_paths = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.mp3')]\n",
        "downloaded_file_paths"
      ],
      "metadata": {
        "id": "XyKx_fUCn0IR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df_new = downloaded_file_paths[:10]\n",
        "final_df_new"
      ],
      "metadata": {
        "id": "pU5L_ydun09b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import dask.bag as db\n",
        "\n",
        "audio_files = db.from_sequence(final_df_new)\n",
        "features_with_ids = audio_files.map(process_audio)\n",
        "mfcc_features_with_ids = features_with_ids.compute()"
      ],
      "metadata": {
        "id": "vAlndXYVn1wt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Create DataFrame\n",
        "features_df = pd.DataFrame(mfcc_features_with_ids, columns=['track_id', 'mfcc'])\n",
        "\n",
        "# Sometimes it's useful to have MFCC features in a format that each column is one feature dimension\n",
        "# This can be done by expanding the mfcc list into separate columns if needed\n",
        "features_df\n"
      ],
      "metadata": {
        "id": "lnA5bTfkn2l0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df = pd.merge(features_df, sampled_df, on='track_id', how='left')"
      ],
      "metadata": {
        "id": "U6uW5AT-n3tM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_df.to_csv('/content/drive/My Drive/ds340/features_df.csv')\n",
        "final_df.to_csv('/content/drive/My Drive/ds340/final_df.csv')"
      ],
      "metadata": {
        "id": "OBZnAj7Yn4hP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_df = pd.read_csv('/content/drive/My Drive/ds340/features_df.csv')\n",
        "\n",
        "features_df"
      ],
      "metadata": {
        "id": "W8LRqpA3n5Pm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "duplicates = filtered_df[filtered_df.duplicated('track_id', keep=False)]\n",
        "\n",
        "duplicates\n"
      ],
      "metadata": {
        "id": "JfT-ShxBoAgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def sort_dataframe_by_column(df, column_name):\n",
        "    # Ensure the DataFrame is sorted by the specified column alphabetically, case-insensitively\n",
        "    sorted_df = df.sort_values(by=column_name, key=lambda col: col.str.lower()).reset_index(drop=True)\n",
        "    return sorted_df\n",
        "\n",
        "\n",
        "sorted_metadata_df = sort_dataframe_by_column(filtered_df, 'track_id')\n",
        "sorted_metadata_df\n"
      ],
      "metadata": {
        "id": "0_w8sVymn6YB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def get_sorted_files(directory_path):\n",
        "    # List all files in the directory\n",
        "    files = os.listdir(directory_path)\n",
        "\n",
        "    # Filter and sort files alphabetically, ensuring case insensitivity\n",
        "    sorted_files = sorted([file for file in files if file.endswith('.mp3')], key=str.lower)\n",
        "\n",
        "    return sorted_files\n",
        "\n",
        "# Specify the path to your director\n",
        "directory_path = '/content/drive/MyDrive/ds340/sampled_files'\n",
        "sorted_files = get_sorted_files(directory_path)\n",
        "sorted_files"
      ],
      "metadata": {
        "id": "5b--2VHcn7Pw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import librosa\n",
        "\n",
        "def load_and_extract_mfcc(file_path, sr=22050, n_mfcc=13, n_fft=2048, hop_length=512):\n",
        "    audio, sample_rate = librosa.load(file_path, sr=sr)\n",
        "    mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=n_mfcc, n_fft=n_fft, hop_length=hop_length)\n",
        "    mfccs = mfccs.T\n",
        "    return mfccs\n",
        "\n",
        "def process_directory(directory_path, metadata_df):\n",
        "    # Sort the metadata DataFrame by track_id alphabetically\n",
        "    sorted_metadata_df = metadata_df.sort_values('track_id', key=lambda x: x.str.lower()).reset_index(drop=True)\n",
        "\n",
        "    # Create a dictionary to map track IDs to genres\n",
        "    file_to_genre = dict(zip(sorted_metadata_df['track_id'], sorted_metadata_df['track_genre']))\n",
        "\n",
        "    # Retrieve and sort list of files in directory alphabetically\n",
        "    files = sorted([f for f in os.listdir(directory_path) if f.endswith('.mp3')], key=str.lower)\n",
        "\n",
        "    mfcc_data = []\n",
        "    for filename in files:\n",
        "        file_path = os.path.join(directory_path, filename)\n",
        "        # Remove '.mp3' to get the track ID\n",
        "        track_id = filename[:-4]  # This strips the last four characters ('.mp3') off the filename\n",
        "        genre = file_to_genre.get(track_id)  # Get the genre for the track ID\n",
        "\n",
        "        if genre is None:\n",
        "            print(f\"Genre not found for {filename}, skipping...\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            mfcc_features = load_and_extract_mfcc(file_path)\n",
        "            mfcc_data.append((track_id, genre, mfcc_features))\n",
        "            print(f\"Processed {filename} with genre {genre}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {filename}: {e}\")\n",
        "\n",
        "    return mfcc_data\n",
        "\n",
        "# Example Usage\n",
        "directory_path = '/content/drive/MyDrive/ds340/sampled_files'\n",
        "processed_data = process_directory(directory_path, filtered_df)\n"
      ],
      "metadata": {
        "id": "163L5ge_n8RG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}